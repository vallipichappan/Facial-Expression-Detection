# Facial-Expression-Detection

In this project, we aim to train an optimal FER model through a series of hypotheses. Overall, we recognise that proper understanding of configurations will substantially improve the efficiency of the model. Hence, we have attempted to explore the different parameters that govern the model architecture as well as to try different transfer learning methods for our project. After selecting the better configurations, we aim to put the model together to build the caption generation model, which is an application of our FER model.

However, recognizing facial expressions is very complex and challenging, as there are expressions that cannot be uniformly classified. This is due to variations in head positions, illuminations, and the fact that unposed expressions are subtle. Thus, this raises the ambiguity in solving the problem due to which the standard accuracy obtained on the famous FER2013 dataset we will be using is limited to 67%, in spite of using CNNs that usually perform very well in image processing problems. Nonetheless, we will aim to identify the best model that can help us to achieve the highest accuracy score that is the closest to the maximum 67% in the most cost-effective way possible. 

## Project Roadmap
- Using the FER2013 dataset, the first part of the project is to first train our baseline CNN model. The aim is to build a simple model that would be used as a standard of comparison for our subsequent models.
- The second part of the project involves experimenting with image augmentation. We believe that applying such techniques can result in improvement in the model performance. The goal is to experiment with different ways of processing or combination of multiple processing, such as random rotation, shifts, shear and flips, etc. This will create some extra randomness in our training dataset and help with the possible overfitting. We will then evaluate the difference in performance by feeding the preprocessed images to the baseline model.
- The third part of the project involves applying transfer learning on the pre-trained models in order to achieve better results. We used pre-trained models - VGG, ResNet and Inception which were trained on millions of facial images. Our goal is to fine-tune these pre-trained models on the FER2013 dataset and compare its performance against the baseline CNN model we have trained.
- The final part of the project involves tuning the parameters of our models, such as experimenting with various activation and optimizer functions, changing the number of layers and introducing dropouts etc. We will study how changes in the respective parameters will affect the performance of our model. Afterwhich, we will obtain the optimal model with the highest accuracy, and we will combine it with an **Caption Generator Model** to demonstrate an application of FER Model.
